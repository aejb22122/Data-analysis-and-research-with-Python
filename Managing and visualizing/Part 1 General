#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""

@author: annick-eudes
"""
# --------------------------------------------------------------------------------------------
#                         DATA ANALYSIS AND VISUALISATION
# --------------------------------------------------------------------------------------------

# ------------------------------- PRELIMINARIES  --------------------------------
#
# First thing set the working directory - it is done by setting it in the folder
# icon to the right;

# Next step is to import all the library we will need

import pandas
import numpy
import seaborn                  # for plots
import matplotlib.pyplot as plt # as plt is sort of a nickname for the library because
                                # it is too long.
import statsmodels.formula.api as smf # statsmodels
import statsmodels.stats.multicomp as multi # statsmodels



# Importing the data set
Data = pandas.read_csv("ool_pds.csv", low_memory = False)

# Because Python is treating the variables has string instead of numeric variables
# we will convert them as numeric with the following function
Data["W1_G2"] = Data["W1_G2"].convert_objects(convert_numeric = True)
Data["W1_P20"] = Data["W1_P20"].convert_objects(convert_numeric = True)
Data["W1_F1"] = Data["W1_F1"].convert_objects(convert_numeric = True)

# The research question is :
# To what extent is the perception of the US situation (W1_G2) associated with the level of income (W1_P20)?

# The variables of interest in our research question
print("W1_P20 is the Personnal Annual income")
print("W1_G2 is the US economy's situation")
print("W1_F1 is the Percentage of how the respondants think about the future")

# Determining the number of rows and columns in the dataset
print("This is the number of observations in the dataset:")
print(len(Data))            # Number of observations = rows

print("This is the number of variables in the dataset:")
print(len(Data.columns))    # Number of variables = columns



# -------------------------- Part # 1 / Basis descriptive data analysis ----------------------------


# ---------------------------- Examination of frequency tables  ------------------------------------

# Explatory data analysis ~ starting with one variable
# Univariate analysis
# The 'dropna = False' argument will display the missing values
# Making simple frequency tables [counts and frequencies].

# Counts :
print("Counts of the us economic situation 1 = Better, 2 = About the same, 3 = Worse, -1 = Refused to answer:")
c1 = Data["W1_G2"].value_counts(sort=False, dropna = False)
print (c1)

print("Count of personal annual income :")
c2 = Data["W1_P20"].value_counts(sort=False, dropna = False)
print (c2)

print("Counts of when you think about your future, are you generally optimistic, pessimistic, or neither optimistic nor pessimistic?")
c3 = Data["W1_F1"].value_counts(sort=False, dropna = False)
print(c3)

# frequencies

print("Percentage of the us economic situation : 1 = Better, 2 = About the same, 3 = Worse, -1 = Refused to answer")
p1 = Data["W1_G2"].value_counts(sort=False, normalize = True)
print (p1)

print("Percentage of personal annual income :")
p2 = Data["W1_P20"].value_counts(sort=False, normalize = True)
print (p2)

print("Percentage of how the respondants think about the future")
p3 = Data["W1_F1"].value_counts(sort=False, normalize = True)
print(p3)


# There are otherways to do this to have the same results, by using the .groupby function
print("Counts of the us economic situation 1 = Better, 2 = About the same, 3 = Worse, -1 = Refused to answer:")
ct1 = Data.groupby("W1_G2").size()
print(ct1)

print("Count of personal annual income :")
ct2 = Data.groupby("W1_P20").size()
print(ct2)

print("Counts of when you think about your future, are you generally optimistic, pessimistic, or neither optimistic nor pessimistic?")
ct3 = Data.groupby("W1_F1").size()
print(ct3)

# To have the frequency, the code is simmilar, we just need to had the *100/len(Data)
print("Percentage of the us economic situation : 1 = Better, 2 = About the same, 3 = Worse, -1 = Refused to answer")
ct4 = Data.groupby("W1_G2").size()*100/len(Data)
print(ct4)

print("Percentage of personal annual income :")
ct5 = Data.groupby("W1_P20").size()*100/len(Data)
print(ct5)

print("Percentage of when you think about your future, are you generally optimistic, pessimistic, or neither optimistic nor pessimistic?")
ct6 = Data.groupby("W1_F1").size()*100/len(Data)
print(ct6)


# ----------------------- Working with a subset of the Data ----------------------------------------

# We can redefine the variables by adding logic statements in the codes.
# We now focus on the economic outcome (considered being better) in the mid level income groups
# i.e. 35,000 < income < 99,999 (For some reason the <= sends to much numbers)

sub1 = Data[(Data["W1_P20"]> 10) & (Data["W1_P20"])< 15 & (Data["W1_G2"] == 1)]
sub2 = sub1.copy()          # the sub1.copy eliminate a copy error that might occure.

# Just to verify
print(len(sub2))            # Number of rows
print(len(Data.columns))    # Number of columns


# Let's look at the frequency tables for the subset for the data frame
print("Counts of the mid-level incomes")
c4 = sub2["W1_P20"].value_counts(sort = False)
print(c4)

print("Counts of repondants that consider that the us economic situation that consider that the country is doing better")
c5 = sub2["W1_G2"].value_counts(sort= False)
print(c5)

print("Percentage of mid-level incomes")
c6 = sub2["W1_P20"].value_counts(sort = False, normalize = True)
print(c6)

print("Percentage of repondants that consider that the us economic situation that consider that the country is doing better")
c7 = sub2["W1_G2"].value_counts(sort = False, normalize = True)
print(c7)


# -------------- PART # 2 / regarding our research ~ Making data management decisions --------------

# We will not be working with a subset of the data, because our research question, and
# our hypothesis, needs to have a view of the optimisum in regards to the income level.
# 1rst we will code the missing values
# we are going to set responses of (-1 ~ Refused)  for these variables to missing,
# so that Python disregards these values. We will code the missing values (nan).

# Data Management

# Data management ~ Making decions about the data
# 1rst decide to code or not the missing values
# Creating or not new variables

# ------------------ Coding or recoding missing values ----------------------------------------

print("Let's start the Data Management ~ decision about the data, missing values", end = '')
print(" and creating secondary variables")
Data["W1_P20"]=Data["W1_P20"].replace(-1, numpy.nan)
Data["W1_G2"]=Data["W1_G2"].replace(-1, numpy.nan)
Data["W1_F1"]=Data["W1_F1"].replace(-1, numpy.nan)

# Let's have a look at the variables with the new managed variables compared to the original variables
# The 'dropna = False' argument will display the missing values

print("Count of personal annual income (with the recoding of the missing values): ")
c2 = Data["W1_P20"].value_counts(sort=False, dropna = False)
print(c2)

print("Counts of When you think about your future, are you generally 1 = optimistic, ",end='')
print(" 2 = neither, or 3 = pessimistic? (with the recoding of the missing values)")
opt = Data["W1_F1"].value_counts(sort=False, dropna = False)
print(opt)

print("Percentage of the us economic situation : 1 = Better, 2 = About the same, ",end='')
print(" 3 = Worse, -1 = Refused to answer (with the recoding of the missing values)")
p1 = Data["W1_G2"].value_counts(sort=False, dropna = True)
print(p1)



# We chose to group values within individual variables for the W1_P20 variable representing
# income level.
# categorize quantitative variable based on customized splits are done by using cut function
# we split the variable into 4 groups (1-7, 8-11, 12-15, 16-19)
# remember that Python starts counting from 0, not 1

# --------------------------- Grouping values within individual variables --------------------------

print("The income level is divided into 4 groups : 1-7 (5k- 24k), 8-11(25k-49k)", end = '')
print(" 12-15(50k-99k), 16-19 (100k-175k or more))")
Data["W1_P20"] = pandas.cut(Data.W1_P20, [0, 7, 11, 15, 19])
c10 = Data["W1_P20"].value_counts(sort = False, dropna = True)
print(c10)


# --------------------------- Counts of the variables ------------------------------
# For verification purposes

print("Counts of the us economic situation 1 = Better, 2 = About the same, ",end='')
print("3 = Worse, -1 = Refused to answer:")
c1 = Data["W1_G2"].value_counts(sort=False)
print (c1)

print("Count of personal annual income :")
c2 = Data["W1_P20"].value_counts(sort=False)
print (c2)

print("Counts of When you think about your future, are you generally 1 = optimistic, ",end='')
print(" 2 = neither, or 3 = pessimistic?")
opt = Data["W1_F1"].value_counts(sort=False)
print(opt)

# These variables have been managed




#  ---------------------------- Part # 3 / Visualising data ~ Graphs -------------------------------

# Visualizing categorical variables
# in order for categorical variables to be ordered properly on the horizontal, or X axis, of
# the univariate graph, we should convert your categorical variables, which are often formatted
# as numeric variables, into a format that Python recognizes as categorical.

# In our research question, we have W1_F1 (view on economic situation) has a categorical variable
# and W1_F1, view of the respondants about the future, and W1_P20 witch is a ordinal variable (Still)
# a type of categorical variable
Data["W1_G2"] = Data["W1_G2"].astype('category')
Data["W1_F1"] = Data["W1_F1"].astype('category')
Data["W1_P20"] = Data["W1_P20"].astype('category')

# Univariate graphics

# Let's plot our categorical variables :
seaborn.countplot(x = "W1_G2", data = Data)
plt.xlabel("-1 = refused, 1 = better, 2 = about the same, or 3 = worse")
plt.title("Respondants views on the nation's economy compared to one year ago")

seaborn.countplot(x = "W1_F1", data = Data)
plt.xlabel("-1 = refused, 1 = optimistic, 2 = neither optimistic nor pessimistic, 3 = pessimistic")
plt.title("Respondants views regarding their future")

seaborn.countplot(x = "W1_P20", data = Data)
plt.xlabel("Interval of annual income :1-7 (5k- 24k), 8-11(25k-49k) 12-15(50k-99k), 16-19 (100k-175k or more)")
plt.title("Income groups reported by respondents")

# Now let's display the graphics for the managed variables
# Graphing a quatitative variable
# The W1_P20 is not a ordinal variable, this is of example only
# seaborn.distplot(Data["W1_P20"].dropna(), kde = False)
# plt.xlabel("Group of personal annual income")
# plt.title("Income groups reported by respondents")

# Standard deviation and other descriptive statistics for quantitative variables
print("Describe the views of the economy's outcome")
desc1 = Data["W1_G2"].describe()
print(desc1)

print("Describe the views on the future by respondants")
desc2 = Data["W1_F1"].describe()
print(desc2)

print("Describe the personnal annual income for the respondants")
desc3 = Data["W1_P20"].describe()
print(desc3)

# ------------------ Make a decision about the role that each variable will play -----
#
# The explanatory variable is the  income level (W1_P20) the perception of the and the response
# variable nationâ€™s economic situation (W1_G2 and/or W1_F1). Thus, using the graphing decisions
# flow chart we will use a Categorical to Categorical bar chart to plot the associations between
# our explanatory and response variables.
# We have to convert the categorical variables to numeric to do a C -> C bar chart.
# Setting variables you will be working with to numeric

Data["W1_P20"] = Data["W1_P20"].convert_objects(convert_numeric=True)
Data["W1_G2"] = Data["W1_G2"].convert_objects(convert_numeric=True)

print("This is the Categorical -> Categorical graph of US economy's situtation vs Personnal annual income")
seaborn.factorplot(x = "W1_P20", y = "W1_G2", data = Data, kind = "bar", ci = None)
plt.xlabel("Personnal annual income")
plt.ylabel("The US economy's situation")

seaborn.factorplot(x = "W1_P20", y = "W1_F1", data = Data, kind = "bar", ci = None)
plt.xlabel("Personnal annual income")
plt.ylabel("How the respondants think about the future ")



# --------------------------------------------------------------------------------------------
#                             DATA ANALYSIS TOOLS
# --------------------------------------------------------------------------------------------

# -------------- To calulate the F-Statistics
# Analysis of variace Quantiative response variable (y) and Explanatory Categorical vairable (x)
# Using ols function for calculating the F-statistic and associated p value
model1 = smf.ols(formula='NUMCIGMO_EST ~ C(MAJORDEPLIFE)', data=sub1)
results1 = model1.fit()
print (results1.summary())

# To verify that the equality of the means
sub2 = sub1[['NUMCIGMO_EST', 'MAJORDEPLIFE']].dropna()

print ('means for numcigmo_est by major depression status')
m1= sub2.groupby('MAJORDEPLIFE').mean()
print (m1)

print ('standard deviations for numcigmo_est by major depression status')
sd1 = sub2.groupby('MAJORDEPLIFE').std()
print (sd1)

# I will call it sub3
sub3 = sub1[['NUMCIGMO_EST', 'ETHRACE2A']].dropna()

model2 = smf.ols(formula='NUMCIGMO_EST ~ C(ETHRACE2A)', data=sub3).fit()
print (model2.summary())

# View the result of the mean comparasion
print ('means for numcigmo_est by major depression status')
m2= sub3.groupby('ETHRACE2A').mean()
print (m2)

print ('standard deviations for numcigmo_est by major depression status')
sd2 = sub3.groupby('ETHRACE2A').std()
print (sd2)

mc1 = multi.MultiComparison(sub3['NUMCIGMO_EST'], sub3['ETHRACE2A'])
res1 = mc1.tukeyhsd()
print(res1.summary())
